
TODO add probability recap

----

#### Probability recap

<font color="#76923c">Joint probability distribution</font> for a set of random variables gives the probability of every atomic  
event on those random variables (i.e., every sample point).
$ğ‘‹, ğ‘Œ, ... , ğ‘$ are random variables with $ğ‘š, ğ‘›, ... , ğ‘$ possible values respectively.

> With the joint probability distribution we can compute the probability of any event, identified by a proposition over the considered random variables

$ğ—$= set of all random variables that define the sample space W  
$ğ˜$= set of random variables in the proposition that defines a particular event  
$ğ‡= ğ— âˆ’ ğ˜$ = set of random variables not part of the definition of the event

$$P(Y=y) = \sum_hP(Y=y, H=h)$$
$$P(Y_1=y_1,...,Y_n=y_n)= \sum_{h_1,...,h_m}P(Y_1=y_1,...,Y_n=y_n, H_1=h_1,...,H_m=h_m)$$
**Example**
ğ— = {ğ‘‡ğ‘œğ‘œğ‘¡â„ğ‘ğ‘â„ğ‘’, ğ¶ğ‘ğ‘£ğ‘–ğ‘¡ğ‘¦}
ğ˜ = {ğ¶ğ‘ğ‘£ğ‘–ğ‘¡ğ‘¦}  
ğ‡ = ğ— âˆ’ ğ˜ = {ğ‘‡ğ‘œğ‘œğ‘¡â„ğ‘ğ‘â„ğ‘’}
$$P(Cavity=true) = \sum_{toothache}P(Cavity=true, toothache)$$$$P(Cavity = true) = P(Cavity = true, Toothache = true)+P(Cavity = true, Toothache =false)$$
##### Conditional probabilities

$ğ—$= set of all random variables that define the sample space W  
$ğ’€$= set of random variables in the proposition that defines a particular  
event (query)  
$ğ„$= set of random variables not in the proposition that defines a particular  
event, but for which values ğ are known (evidence)  
$ğ‡= ğ— âˆ’ ğ˜ âˆ’ ğ„$= set of remaining random variables for which we donâ€™t  
know the values (hidden variables)
$$P(Y|E =e)=\alpha P(Y,E=e)=\alpha \sum_hP(Y,E=e,H=h)$$**Example**
ğ— = {ğ‘‡ğ‘œğ‘œğ‘¡â„ğ‘ğ‘â„ğ‘’, ğ¶ğ‘ğ‘£ğ‘–ğ‘¡ğ‘¦, ğ¶ğ‘ğ‘¡ğ‘â„}
ğ˜ = {ğ¶ğ‘ğ‘£ğ‘–ğ‘¡ğ‘¦}  
ğ„ = ğ‘‡ğ‘œğ‘œğ‘¡â„ğ‘ğ‘â„ğ‘’ ; ğ = {ğ‘¡ğ‘Ÿğ‘¢ğ‘’}  
ğ‡ = ğ— âˆ’ ğ˜ âˆ’ ğ„ = {ğ¶ğ‘ğ‘¡ğ‘â„}
$$P(ğ¶ğ‘ğ‘£ğ‘–ğ‘¡ğ‘¦|ğ‘‡ğ‘œğ‘œğ‘¡â„ğ‘ğ‘â„ğ‘’) = ğ‘¡ğ‘Ÿğ‘¢ğ‘’ = ğ›¼ğ(ğ¶ğ‘ğ‘£ğ‘–ğ‘¡ğ‘¦, ğ‘‡ğ‘œğ‘œğ‘¡â„ğ‘ğ‘â„ğ‘’= ğ‘¡ğ‘Ÿğ‘¢ğ‘’ )= ğ›¼\sum_{Catch}ğ(ğ¶ğ‘ğ‘£ğ‘–ğ‘¡ğ‘¦, ğ‘‡ğ‘œğ‘œğ‘¡â„ğ‘ğ‘â„ğ‘’ = ğ‘¡ğ‘Ÿğ‘¢ğ‘’, ğ¶ğ‘ğ‘¡ğ‘â„)=  
= ağ (ğ¶ğ‘ğ‘£ğ‘–ğ‘¡ğ‘¦, ğ‘‡ğ‘œğ‘œğ‘¡â„ğ‘ğ‘â„ğ‘’ = ğ‘¡ğ‘Ÿğ‘¢ğ‘’, ğ¶ğ‘ğ‘¡ğ‘â„ = ğ‘¡ğ‘Ÿğ‘¢ğ‘’) +  
ğ(ğ¶ğ‘ğ‘£ğ‘–ğ‘¡ğ‘¦, ğ‘‡ğ‘œğ‘œğ‘¡â„ğ‘ğ‘â„ğ‘’ = ğ‘¡ğ‘Ÿğ‘¢ğ‘’, ğ¶ğ‘ğ‘¡ğ‘â„ = ğ‘“ğ‘ğ‘™ğ‘ ğ‘’))$$

##### Independence
$$P(A|B) = P(A) or P(B|A)= P(B) or P(A,B)=P(A)*P(B)$$
##### Conditional independence
$A$ is conditionally independent of $C$ given $B$.
- $P(A|B,C) = P(A|B)$
- $P(C|A,B) = P(C|B)$
- $P(A,C|B) = P(A|B)P(C|B)$

We can use <font color="#76923c">chain rule</font> to represent joint probability distributions.  
Chain rule is derived by successive application of product rule.

---
#### Bayesian Networks

> A simple, graphical notation for conditional independence assertions and hence for compact specification of full joint distributions.

Syntax:
- a set of nodes, one per variable
- directed links with no directed cycles $\rightarrow$ directed, acyclic graph, (link ~ 'directly' influences)
- a conditional distribution for each node given its parents:
	- $P(X_i|Parents(X_i))$

In the simplest case, conditional distribution is represented as a <font color="#76923c">conditional probability table (CPT)</font> giving the distribution over $ğ‘‹_i$ for each combination of parent values.

----
##### Example

> **1**

Topology of network encodes conditional independence assertions:

![[Screenshot 2023-12-04 at 2.00.09â€¯PM.png|300]]

ğ‘Šğ‘’ğ‘ğ‘¡â„ğ‘’ğ‘Ÿ is independent of the other variables.
ğ‘‡ğ‘œğ‘œğ‘¡â„ğ‘ğ‘â„ğ‘’ and ğ¶ğ‘ğ‘¡ğ‘â„ are conditionally independent given ğ¶ğ‘ğ‘£ğ‘–ğ‘¡ğ‘¦.

>**2**

I'm at work, neighbour John calls to say my alarm is ringing, but neighbour Mary doesnâ€™t call. Sometimes alarm is set off by minor earthquakes. Is there a burglar?

<u>Variables</u>: ğµğ‘¢ğ‘Ÿğ‘”ğ‘™ğ‘ğ‘Ÿ, ğ¸ğ‘ğ‘Ÿğ‘¡â„ğ‘ğ‘¢ğ‘ğ‘˜ğ‘’, ğ´ğ‘™ğ‘ğ‘Ÿğ‘š, ğ½ğ‘œâ„ğ‘›ğ¶ğ‘ğ‘™ğ‘™ğ‘ , ğ‘€ğ‘ğ‘Ÿğ‘¦ğ¶ğ‘ğ‘™ğ‘™ğ‘   

Network topology reflects â€œcausalâ€ knowledge:  
- A burglar can set the alarm off  
- An earthquake can set the alarm off  
- The alarm can cause Mary to call  
- The alarm can cause John to call  

![[Screenshot 2023-12-04 at 2.02.33â€¯PM.png|350]]

Laziness + ignorance cause some factors to not be modeled as nodes, but they are implicitly summarized in the uncertainty associated with links. E.g. John sometimes confuses the phone ringing with the alarm.

----
#### Compactness

> A <font color="#76923c">CPT</font> for Boolean $ğ‘‹_i$ with ğ‘˜ Boolean parents has $2^k$ rows for the combinations of parent values.

Each row requires one number ğ‘ for $ğ‘‹_i$ = ğ‘¡ğ‘Ÿğ‘¢ğ‘’ (the number for $ğ‘‹_i$ = ğ‘“ğ‘ğ‘™ğ‘ ğ‘’ is just 1 âˆ’ ğ‘)
If each variable has no more than ğ‘˜ parents, the complete network requires  $ğ‘‚(ğ‘› *2^k)$ numbers for ğ‘› vars.
I.e., grows linearly with ğ‘›, vs. $ğ‘‚(2^n)$ for the full joint distribution.
For burglary net, 1 + 1 + 4 + 2 + 2 = 10 numbers (vs. $2^5 âˆ’ 1 = 31$)

---
#### Semantics

##### Global

<font color="#76923c">Global</font> semantics defines the full joint distribution as the product of the local conditional distributions:
$$P(X_1,...,X_n)= \Pi_{i=1}^nP(X_i|Parents(X_i))$$
$\rightarrow$ a Bayesian Network can answer any query!

**Example**

$P(j\wedge m\wedge a\wedge \neg b\wedge \neg e)= P(j|a)P(m|a)P(a|\neg b,\neg e)P(\neg b)P(\neg e)= 0.9 Ã—0.7Ã—0.001Ã—0.999Ã—0.998\approx 0.00063$

![[Screenshot 2023-12-04 at 2.08.32â€¯PM.png|350]]

##### Local

<font color="#76923c">Local</font> semantics: each node is independent of its nondescendants given its parents.

![[Screenshot 2023-12-04 at 2.21.19â€¯PM.png|300]]

- descendant = child node or descendant of child node  
- nondescendant = any node that is not a descendant  

- $B$ independent of $E$ given $\emptyset$
	- B is unconditionally independent of E
- $E$ independent of $B$ given $\emptyset$
	- E is unconditionally independent of B
- $J$ independent of {$B,E, M$} given {$A$}
	- Also: ğ½ independent of ğµ given ğ´,  
	- ğ½ independent of ğ¸ given ğ´, etc
- $M$ independent of {$B,E, J$} given {$A$}
- $A$ independent of..?

![[Screenshot 2023-12-04 at 2.25.07â€¯PM.png|250]]

----
#### Markov's blanket

Another independence property: each node is conditionally independent of all others given its Markov blanket: parents + children + childrenâ€™s parents.

![[Screenshot 2023-12-04 at 2.29.33â€¯PM.png|300]]

---
#### Constructing Bayesian Networks

Need a method such that a series of locally testable assertions of conditional independence guarantees the required global semantics.
1. Choose an ordering of variables $ğ‘‹_1 , ... , ğ‘‹_n$
2. For ğ‘– = 1 to ğ‘›
	1. add $ğ‘‹_i$ to the network
		1. select parents from $ğ‘‹_1 , ... , ğ‘‹_{i-1}$ such that $P(X_i|Parents(X_i))=P(X_i|X_1,...,X_{i=1})$

This choice of parents guarantees the global semantics:
$P(X_1,...,X_n)=\Pi_{i=1}^nP(X_i|X_1,...,X_{i-1})=\Pi_{i=1}^nP(X_i|Parents(X_i))$

**Example**
Suppose we choose the ordering $M,J,A,B,E$
$P(J|M)=P(J)$ ? no
$P(A|J,M)=P(A|J)?P(A|J,M)=P(A)$ ? no
$P(B|A,J,M)=P(B|A)$ ? yes
$P(B|A,J,M)=P(B)$ ? no
$P(E|B,A,J,M)=P(E|A)$ ? no
$P(E|B,A,J,M) = P(E|A,B)$ ? yes

![[Screenshot 2023-12-04 at 5.01.51â€¯PM.png|200]]

Deciding conditional independence is hard in noncausal directions  
Causal models and conditional independence seem hardwired for humans!  
Assessing conditional probabilities is hard in noncausal directions  
Network is less compact: 1 + 2 + 4 + 2 + 4 = 13 numbers needed

---
#### Compact conditional distributions

CPT grows exponentially with number of parents  
CPT becomes infinite with continuous-valued parent or child

Solution: <font color="#76923c">canonical</font> distributions that are defined compactly

<font color="#76923c">Deterministic</font> nodes are the simplest case:  
ğ‘‹ = ğ‘“(ğ‘ƒğ‘ğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘ (ğ‘‹)) for some function ğ‘“  

E.g., Boolean functions  
ğ‘ğ‘œğ‘Ÿğ‘¡â„ğ´ğ‘šğ‘’ğ‘Ÿğ‘–ğ‘ğ‘ğ‘› $\leftrightarrow$ ğ¶ğ‘ğ‘›ğ‘ğ‘‘ğ‘–ğ‘ğ‘› $\leftrightarrow$  ğ‘ˆğ‘† $\leftrightarrow$ ğ‘€ğ‘’ğ‘¥ğ‘–ğ‘ğ‘ğ‘›  

E.g., numerical relationships among continuous variables  
$\frac{ğœ•ğ¿ğ‘’ğ‘£ğ‘’ğ‘™}{ğœ•ğ‘¡} = ğ‘–ğ‘›ğ‘“ğ‘™ğ‘œğ‘¤ + ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ğ‘–ğ‘¡ğ‘ğ‘¡ğ‘–ğ‘œğ‘› âˆ’ ğ‘œğ‘¢ğ‘¡ğ‘“ğ‘™ğ‘œğ‘¤ âˆ’ ğ‘’ğ‘£ğ‘ğ‘ğ‘œğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘œğ‘›$

Noisy-OR distributions model multiple causes  
1. Parents $ğ‘ˆ_1, ... ,U_k$ include all causes (can add leak node)  
2. Independent failure probability $q_i$ for each cause alone
$\rightarrow$ $P(x_i|parents(X_i))=1-\Pi_{j:X_j=true\in parents(X_i)}q_j$

![[Screenshot 2023-12-04 at 5.12.01â€¯PM.png|300]]
Number of parameters <font color="#76923c">linear</font> in number of parents

---
#### Hybrid (discrete+continuous) networks

Discrete: ğ‘†ğ‘¢ğ‘ğ‘ ğ‘–ğ‘‘ğ‘¦? and ğµğ‘¢ğ‘¦ğ‘ ?  
Continuous: ğ»ğ‘ğ‘Ÿğ‘£ğ‘’ğ‘ ğ‘¡ and ğ¶ğ‘œğ‘ ğ‘¡

![[Screenshot 2023-12-04 at 5.14.07â€¯PM.png|200]]

> How to deal with this?  

<u>Option 1</u>: discretization â€“ possibly large errors, large CPTs  
<u>Option 2:</u> define standard families of probability density functions with finite number of parameters, e.g. Gaussian distribution  

For hybrid nets we need to specify new kinds of conditional distr.:  
1) Continuous variable, discrete + continuous parents (e.g., ğ¶ğ‘œğ‘ ğ‘¡)  
2) Discrete variable, continuous parents (e.g., ğµğ‘¢ğ‘¦ğ‘ ?)

1. **Continuous child variables:**

For each possible assignment to discrete parents (i.e. ğ‘†ğ‘¢ğ‘ğ‘ ğ‘–ğ‘‘ğ‘¦): need one <font color="#76923c">conditional</font>  
density function for child variable (i.e. ğ¶ğ‘œğ‘ ğ‘¡) given continuous parents (i.e. ğ»ğ‘ğ‘Ÿğ‘£ğ‘’ğ‘ ğ‘¡).

2. **Discrete variables with continuous parents:**

Probability of ğµğ‘¢ğ‘¦ğ‘ ? given ğ¶ğ‘œğ‘ ğ‘¡ should be a â€œsoftâ€ threshold:

![[Screenshot 2023-12-04 at 5.18.44â€¯PM.png|300]]

---
#### Summary

- Bayes nets provide a natural representation for (causally induced) conditional  independence  
- Topology + CPTs = compact representation of joint distribution  
- Generally easy for (non)experts to construct  
- Canonical distributions (e.g., noisy-OR) = compact representation of CPTs  
- Continuous variables â‡’ parameterized distributions